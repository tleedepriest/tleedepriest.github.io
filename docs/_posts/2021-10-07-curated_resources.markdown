---
layout: post
title:  "Curated Resources"
date:   2022-10-07
categories: jekyll update
---

# Introduction

In my free time, I have been educating myself to become a Data Engineer. I am currently a Research Associate at a Federally Funded Research & Development Center where I spend my time completing various tasks, but mainly building automated pipelines for reproducible research.

Below are topics I have stumbled upon, along with the **free resources** I found and use as I create projects and tutorials in my self-learning journey.

## GNU Make

GNU Make is shell scripts on steroids. If you have ever found yourself commenting lines in shell/bash script, only to uncomment them again, this tool if for you. The concepts in Make underly more advanced tools, such as [Luigi](https://luigi.readthedocs.io/en/stable/) and [Airflow](https://airflow.apache.org/), so whether or not you end up using this tool, learning it will be beneficial if you want to create data pipelines. 

This was my first introduction into the world of automation. The resource below is the best  introduction I have found on the subject, and I regularly reference it for a refresher.

- Resource
  - [Automation and Make](https://swcarpentry.github.io/make-novice/) from the software carpentry group! (They have a few great tutorials!)

## SQL 

- A create introduction to SQL if you have never done a SELECT * FROM statement, completed JOINs, or even for more advanced functions. This is likely to Cater more to an Analysts, but is the best Comprehensive Introduction to SQL that I have found. This 
  - [SQL for data analysis](https://www.udacity.com/course/sql-for-data-analysis--ud198)

- Resources for working with SQLite using python
  - [Storing Data With SQLite](https://www.pythonforthelab.com/blog/storing-data-with-sqlite/) from pythonforthelab
  - [Databases and SQL](http://swcarpentry.github.io/sql-novice-survey/)
  - [SQLite Guide](https://pynative.com/python-sqlite/) comprehensive guide to using SQLite 

## Luigi

- [Building Data Pipelines with Python and Luigi](https://marcobonzanini.com/2015/10/24/building-data-pipelines-with-python-and-luigi/)

## AWS

- Eventually, you are going to need to stop coding locally and push your code to EC2 instances in the cloud, store your data in S3 buckets, and more. You will first need to sign up for a [Free Tier AWS account](https://aws.amazon.com/free/?all-free-tier.sort-by=item.additionalFields.SortRank&all-free-tier.sort-order=asc&awsf.Free%20Tier%20Types=*all&awsf.Free%20Tier%20Categories=*all)
- Next, after the signup, you can access free tutorials which I highly recommend going through and completing, as they are generally prerequisites to any other more advanced tutorial.

#### Terraform

- I was looking for a solution to run my data-pipeline in the cloud, and store the results in an S3 Bucket. Here is a post I found on reddit [r/dataengineering](reddit.com/r/dataengineering) 

- [Correct method of Setting Up/Initializing AWS](https://www.reddit.com/r/dataengineering/comments/q2m97l/correct_method_of_setting_upinitializing_aws/)

  

  

  
